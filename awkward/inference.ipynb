{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 두 문장 단위 인퍼런스\n",
    "**파일명 : inference.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "from torchtext import data\n",
    "from engine.models.awkward import awkwardClassifier\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "path = \"./\"\n",
    "#model_name = \"model_gpu_nsp_cosine_v7.pth\"\n",
    "#model_name = \"model_gpu_okt_layer2_seq64_v8.pth\"\n",
    "model_name = \"model_gpu_okt_layer2_v9.pth\"\n",
    "\n",
    "use_eos=False\n",
    "n_classes = 2\n",
    "\n",
    "saved_data = torch.load(path + model_name, map_location = 'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = saved_data['config']\n",
    "awkward_best = saved_data['awkward']\n",
    "vocab1 = saved_data['vocab1']\n",
    "vocab2 = saved_data['vocab2']\n",
    "classes = saved_data['classes']\n",
    "\n",
    "vocab1_size = len(vocab1)\n",
    "vocab2_size = len(vocab2)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def infer(x1, x2):\n",
    "    \n",
    "    def define_field():\n",
    "\n",
    "#         return (\n",
    "#             data.Field(sequential = True,\n",
    "#                                   use_vocab = True,\n",
    "#                                   batch_first = True,\n",
    "#                                   include_lengths = False,\n",
    "#                                   eos_token='<EOS>' if use_eos else None,\n",
    "#                                   pad_first=True\n",
    "#                                   ),\n",
    "#             data.Field(sequential = True,\n",
    "#                                   use_vocab = True,\n",
    "#                                   batch_first = True,\n",
    "#                                   include_lengths = False,\n",
    "#                                   eos_token='<EOS>' if use_eos else None,\n",
    "#                                   pad_first=True\n",
    "#                                   ),        \n",
    "#             data.Field(sequential = False,\n",
    "#                                  preprocessing = lambda x: int(x),\n",
    "#                                      use_vocab = True,\n",
    "#                                     init_token = None,\n",
    "#                                      unk_token = None\n",
    "#                                   )\n",
    "#         )\n",
    "\n",
    "        return (data.Field(sequential = True,\n",
    "                              tokenize=okt.morphs, # data가 이미 tokenizing 되어 있음.\n",
    "                              use_vocab = True,\n",
    "                              batch_first = True,\n",
    "                              include_lengths = False,\n",
    "                              eos_token='<EOS>' if use_eos else None,\n",
    "                              pad_first=True\n",
    "                              ),\n",
    "                 data.Field(sequential = True,\n",
    "                              tokenize=okt.morphs,\n",
    "                              use_vocab = True,\n",
    "                              batch_first = True,\n",
    "                              include_lengths = False,\n",
    "                              eos_token='<EOS>' if use_eos else None,\n",
    "                              pad_first=True\n",
    "                              ),\n",
    "        # sequential : If False,no tokenization is applied\n",
    "        data.Field(sequential = False,\n",
    "                             preprocessing = lambda x: int(x),\n",
    "                                 use_vocab = True,\n",
    "                                init_token = None,\n",
    "                                 unk_token = None\n",
    "                               )\n",
    "    )\n",
    "    \n",
    "\n",
    "    text1_field, text2_field, label_field = define_field()\n",
    "    \n",
    "    text1_field.vocab = vocab1\n",
    "    text2_field.vocab = vocab2 \n",
    "    label_field.vocab = classes\n",
    "    \n",
    "    lines1 = []\n",
    "    lines2 = []\n",
    "    \n",
    "    lines1 += [x1.strip().split(' ')]\n",
    "    lines2 += [x2.strip().split(' ')]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x1 = text1_field.numericalize(\n",
    "        text1_field.pad(lines1),\n",
    "        device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "        )\n",
    "\n",
    "        x2 = text2_field.numericalize(\n",
    "        text2_field.pad(lines2),\n",
    "        device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "        )\n",
    "\n",
    "        model = awkwardClassifier(\n",
    "                ntoken = vocab1_size,\n",
    "                ntoken2 = vocab2_size,\n",
    "                ninp = config.word_vec_size,\n",
    "                n_classes=n_classes,\n",
    "                nhead = config.nhead, \n",
    "                nhid = config.nhid, \n",
    "                nlayers = config.nlayers,\n",
    "                cos = config.cos,\n",
    "                dropout=config.dropout,\n",
    "            )\n",
    "        \n",
    "        ensemble = []\n",
    "        model.load_state_dict(awkward_best)\n",
    "        ensemble += [model]\n",
    "        \n",
    "        if config.gpu_id >= 0:\n",
    "            model.cuda(config.gpu_id)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        y_hat = []\n",
    "        \n",
    "        def pad_to_maxseq_to_batch(batch, max_length, device=-1):\n",
    "    \n",
    "            if batch.size(1) >= max_length:\n",
    "                batch = batch[:, :max_length]\n",
    "\n",
    "            else:\n",
    "                pad_num = max_length - batch.size(1)\n",
    "                pad = torch.ones(batch.size(0), pad_num, device=device) # pad 값이 vocab index 1이라는 가정.\n",
    "                batch = torch.cat([pad.long(), batch], dim=-1)\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        \n",
    "    x1 = pad_to_maxseq_to_batch(x1, config.max_length, device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    x2 = pad_to_maxseq_to_batch(x2, config.max_length, device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(model(x1, x2)[0])    \n",
    "    return torch.argmax(model(x1, x2)[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : 비정상\n",
    "# 0 : 정상\n",
    "import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = \"실제로 저는 조발표가 있는 수업을 좋아하지 않았고 특히 발표자 역할은 맡으려고 하지 않았습니다.\"\n",
    "line2 = \"하지만 프레젠테이션 능력은 어디를 가도 꼭 필요한 것임을 인지하게 되었고 이후에는 이를 개선하기 위해 노력했습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.infer(line1, line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 자기소개서 단건 Inferenece\n",
    "**파일명 : inference_resume.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_resume.py\n",
    "\n",
    "from torchtext import data\n",
    "from engine.models.awkward import awkwardClassifier\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import torch\n",
    "import kss\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "path = \"./\"\n",
    "#model_name = \"model_gpu_nsp_cosine_v7.pth\"\n",
    "model_name = \"model_gpu_okt_layer2_seq64_v8.pth\"\n",
    "#model_name = \"model_gpu_okt_layer2_v9.pth\"\n",
    "\n",
    "use_eos=False\n",
    "n_classes = 2\n",
    "\n",
    "saved_data = torch.load(path + model_name, map_location = 'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = saved_data['config']\n",
    "awkward_best = saved_data['awkward']\n",
    "vocab1 = saved_data['vocab1']\n",
    "vocab2 = saved_data['vocab2']\n",
    "classes = saved_data['classes']\n",
    "\n",
    "vocab1_size = len(vocab1)\n",
    "vocab2_size = len(vocab2)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def infer(resume):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        def define_field():\n",
    "\n",
    "            return (data.Field(sequential = True,\n",
    "                                  tokenize=okt.morphs, # data가 이미 tokenizing 되어 있음.\n",
    "                                  use_vocab = True,\n",
    "                                  batch_first = True,\n",
    "                                  include_lengths = False,\n",
    "                                  eos_token='<EOS>' if use_eos else None,\n",
    "                                  pad_first=True\n",
    "                                  ),\n",
    "                     data.Field(sequential = True,\n",
    "                                  tokenize=okt.morphs,\n",
    "                                  use_vocab = True,\n",
    "                                  batch_first = True,\n",
    "                                  include_lengths = False,\n",
    "                                  eos_token='<EOS>' if use_eos else None,\n",
    "                                  pad_first=True\n",
    "                                  ),\n",
    "            # sequential : If False,no tokenization is applied\n",
    "            data.Field(sequential = False,\n",
    "                                 preprocessing = lambda x: int(x),\n",
    "                                     use_vocab = True,\n",
    "                                    init_token = None,\n",
    "                                     unk_token = None\n",
    "                                   )\n",
    "        )\n",
    "\n",
    "        text1_field, text2_field, label_field = define_field()\n",
    "\n",
    "        text1_field.vocab = vocab1\n",
    "        text2_field.vocab = vocab2 \n",
    "        label_field.vocab = classes\n",
    "\n",
    "        lines = kss.split_sentences(resume)\n",
    "\n",
    "        line_len = len(lines)\n",
    "\n",
    "        first_resume  = lines[:-1]\n",
    "        second_resume = lines[1:]\n",
    "\n",
    "        lines1 = []\n",
    "        lines2 = []\n",
    "\n",
    "        for i in first_resume:\n",
    "            lines1 += [i.strip().split(' ')]\n",
    "\n",
    "        for i in second_resume:\n",
    "            lines2 += [i.strip().split(' ')]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x1 = text1_field.numericalize(\n",
    "            text1_field.pad(lines1),\n",
    "            device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "            )\n",
    "\n",
    "            x2 = text2_field.numericalize(\n",
    "            text2_field.pad(lines2),\n",
    "            device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "            )\n",
    "\n",
    "            model = awkwardClassifier(\n",
    "                    ntoken = vocab1_size,\n",
    "                    ntoken2 = vocab2_size,\n",
    "                    ninp = config.word_vec_size,\n",
    "                    n_classes=n_classes,\n",
    "                    nhead = config.nhead, \n",
    "                    nhid = config.nhid, \n",
    "                    nlayers = config.nlayers,\n",
    "                    cos = config.cos,\n",
    "                    dropout=config.dropout,\n",
    "                )\n",
    "\n",
    "            ensemble = []\n",
    "            model.load_state_dict(awkward_best)\n",
    "            ensemble += [model]\n",
    "\n",
    "            if config.gpu_id >= 0:\n",
    "                model.cuda(config.gpu_id)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y_hat = []\n",
    "\n",
    "            def pad_to_maxseq_to_batch(batch, max_length, device=-1):\n",
    "\n",
    "                if batch.size(1) >= max_length:\n",
    "                    batch = batch[:, :max_length]\n",
    "\n",
    "                else:\n",
    "                    pad_num = max_length - batch.size(1)\n",
    "                    pad = torch.ones(batch.size(0), pad_num, device=device) # pad 값이 vocab index 1이라는 가정.\n",
    "                    batch = torch.cat([pad.long(), batch], dim=-1)\n",
    "\n",
    "                return batch\n",
    "\n",
    "        x1 = pad_to_maxseq_to_batch(x1, config.max_length, device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        x2 = pad_to_maxseq_to_batch(x2, config.max_length, device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        result = torch.argmin(model(x1, x2), dim=-1)\n",
    "        result_array = result.to(torch.device(\"cpu\")).numpy()\n",
    "        \n",
    "        result_sentence = []\n",
    "        \n",
    "        for sen, result in zip(second_resume, result_array):\n",
    "            if result == 0: # 이상문장일 경우\n",
    "                result_sentence.append(sen)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(\"ERROR : \",e)\n",
    "    \n",
    "    return result_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 자기소개서는 기본적인 전처리가 된 상태여야 한다.\n",
    "resume = \"삼성전자 인턴 당시 저는 인턴 총무를 맡았습니다. 코로나 언제 끝나지. 총무를 맡았던 당시 잦은 인턴 회식으로 회비가 생각보다 많이 지출 되었습니다. 그래서 일부에서는 총무가 너무 헤프게 돈을 관리한다고 지적이 나왔었습니다. 그래서 저는 영수증과 앞으로의 계획을 문서화하여 논란이 있었던 부분을 해결하고자 하였습니다.먼저 투명한 예산공개를 통해 동기 인턴들에게 신뢰를 주었습니다. 그리고 불만을 말했던 동기들에게 먼저 다가가 사실관계를 설명 후 각자의 입장에 대해 이해하도록 노력하였습니다. 그리고 마지막으로 앞으로의 계획을 말하여 동기 전체의 이해와 스케줄관리를 하였습니다.저는 이런 갈등관계에서 소통이라는 해결방법을 선택하여, 갈등관계를 해결하고자 노력하였습니다. 이러한 방법은 서로의 입장을 이해 할 수 있었고, 다른 조직생활과 현재 근무지에서도 업무를 처리할 때에도 트러블이 나지 않고 좋은 관계를 유지중입니다.롯데 입사 후에도 상대방의 생각을 이해하고 소통을 통하여 좋은 관계를 유지하여 조직적 성과를 낼 수 있도록 노력하겠습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "result = inference_resume.infer(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['코로나 언제 끝나지.',\n",
       " '먼저 투명한 예산공개를 통해 동기 인턴들에게 신뢰를 주었습니다.',\n",
       " '그리고 불만을 말했던 동기들에게 먼저 다가가 사실관계를 설명 후 각자의 입장에 대해 이해하도록 노력하였습니다.',\n",
       " '이러한 방법은 서로의 입장을 이해 할 수 있었고, 다른 조직생활과 현재 근무지에서도 업무를 처리할 때에도 트러블이 나지 않고 좋은 관계를 유지중입니다.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.5 on Python 3.6 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
